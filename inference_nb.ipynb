{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Routine from a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 453\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "from src.datasetComposer import DatasetBuilder, composed_train_path, composed_test_path, compactComposer, test_path, train_path, test_path,setupTokenizer\n",
    "from src.inference_routine import InferenceGenerator\n",
    "from src.datasetHandlers import SmartCollator\n",
    "from src.model_utils import get_basic_model\n",
    "from src.trainerArgs import CustomTrainer, getTrainingArguments\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "\n",
    "iterative_gen = True\n",
    "composed_already = True\n",
    "\n",
    "# Define the parameters used to set up the models\n",
    "modeltype = 'iterative' if iterative_gen else 'normal'  # either baseline or 'earlyfusion'\n",
    "\n",
    "# either t5-small,t5-base, t5-large, facebook/bart-base, or facebook/bart-large\n",
    "modelbase = 'facebook/bart-base'\n",
    "\n",
    "# we will use the above variables to set up the folder to save our model\n",
    "pre_trained_model_name = modelbase.split(\n",
    "    '/')[1] if 'bart' in modelbase else modelbase\n",
    "\n",
    "# where the trained model will be saved\n",
    "output_path = 'TrainModels/' + modeltype + '/'+pre_trained_model_name+'/'\n",
    "\n",
    "#tests = json.load(open(test_path,encoding='utf-8'))\n",
    "\n",
    "\n",
    "rand_seed = 453\n",
    "seed_everything(rand_seed)\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "arguments = train_arguments = {'output_dir': output_path,\n",
    "                               'warmup_ratio': 0.2,\n",
    "                               #'disable_tqdm':False,\n",
    "                               'per_device_train_batch_size': 8,\n",
    "                               'num_train_epochs': 4,\n",
    "                               'lr_scheduler_type': 'cosine',\n",
    "                               'learning_rate': 5e-5,\n",
    "                               'evaluation_strategy': 'steps',\n",
    "                               'logging_steps': 500,\n",
    "                               \n",
    "                               'seed': rand_seed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the modules from the inference routine\n",
    "from types import SimpleNamespace\n",
    "from src.inference_routine import NarratorUtils,ExplanationRecord,LocalLevelExplanationNarration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dont forget to call initialise_Model() before running any inference\n"
     ]
    }
   ],
   "source": [
    "\n",
    "narrator_utils = NarratorUtils(modelbase,output_path)\n",
    "\n",
    "# initialise the model\n",
    "classification_explanator = narrator_utils.initialise_Model()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import random\n",
    "# Example of input\n",
    "\n",
    "ml_task_name = 'Car Insurance Risk'\n",
    "\n",
    "prediction_probabilities = {'Low': 0.76, 'High': 0.24}\n",
    "# the features used to make the prediction\n",
    "feature_names = ['Height', 'Mar_status', 'cur_loc', 'nb_friends', 'last_trip']\n",
    "\n",
    "bcc = feature_names.copy()\n",
    "random.shuffle(bcc)\n",
    "\n",
    "# get the order and directions of influence from the explainable output from the XAI technique\n",
    "# the methods expects the keys ['explanation_order','positives','negatives','ignore']\n",
    "# 'positives' is the list of all the features with positive influence on the prediction decision and 'negatives' is the inverse.\n",
    "# 'ignore' is the list of features identified as having very limited contribution to the prediction decision\n",
    "\n",
    "attributions = {'explanation_order': ['Height', 'last_trip', 'cur_loc', 'nb_friends', 'Mar_status','Income'],\n",
    "                'positives': ['Height', 'last_trip', 'Mar_status'],\n",
    "                'negatives': ['cur_loc', 'nb_friends'],\n",
    "                'ignore': ['Income']\n",
    "                \n",
    "                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to generate the texts via the iterative generation then we have to define the style\n",
    "# We want our output text to first table about the prediction output\n",
    "# step 1: talk about the feature order based on the attributions\n",
    "# step 2: talk about the features with positive contributions to the decision\n",
    "# step 3: ----- negative features\n",
    "# step 4: ------- features with limited influence\n",
    "# step 5: Make conclusion based on all the input information\n",
    "\n",
    "# this will instruct the narrator to follow our desired output style\n",
    "iterative_generation_steps = {'step 0': '',\n",
    "                              'step 1': attributions['explanation_order'],\n",
    "                              'step 5': attributions['ignore'],\n",
    "                              'step 2': attributions['positives'][:2],\n",
    "                              'step 3': attributions['negatives'][:1],\n",
    "                              'step 4': attributions['negatives'][1:] + attributions['positives'][2:],\n",
    "                              \n",
    "                              'step 6': '-'\n",
    "                              }\n",
    "\n",
    "\n",
    "full_text_generation_steps = {'step 0': '',\n",
    "                              'step 1': attributions['explanation_order'],\n",
    "                              }\n",
    "\n",
    "\n",
    "generation_instruction = iterative_generation_steps if iterative_gen else full_text_generation_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ML model predicted the label : Low\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ML model predicted the label : Low\n"
     ]
    }
   ],
   "source": [
    "# Process the explanation output and the text generation instruction\n",
    "exp_record = ExplanationRecord(ml_task_name,feature_names,prediction_probabilities, attributions,iterative_mode=True)\n",
    "processed = exp_record.setup_generation_steps(generation_instruction,)\n",
    "\n",
    "# the final bit \n",
    "iterativeGen =LocalLevelExplanationNarration(classification_explanator,narrator_utils,device,iterative_mode=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"According to the model, the most probable label for the given case is Low with a probability of 76.0%, which is higher than that of  High. It is important to note that the classifier is very uncertain about the correctness of the assigned label and this is mainly because the likelihoods across the classes are not equal to zero. Among the input features, only Height, last_trip, and cur_loc are shown to have a positive influence on the model's prediction decision, while nb_friends has a negative influence, shifting the verdict away from Low. Income, on the other hand, has a negative influence, shifting the verdict away from Low towards  High.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterativeGen.generateTexts(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('annotation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4aea246828f75a58a93204fce55d322b87a38415c2742fb8a88040418150f4d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
