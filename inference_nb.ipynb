{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Routine from a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "from src.datasetComposer import DatasetBuilder, composed_train_path, composed_test_path, compactComposer, test_path, train_path, test_path,setupTokenizer\n",
    "from src.inference_utils import InferenceGenerator\n",
    "from src.datasetHandlers import SmartCollator\n",
    "from src.model_utils import get_basic_model\n",
    "from src.trainerArgs import CustomTrainer, getTrainingArguments\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iterative_gen = True\n",
    "composed_already = True\n",
    "\n",
    "# Define the parameters used to set up the models\n",
    "modeltype = 'iterative' if iterative_gen else 'normal'  # either baseline or 'earlyfusion'\n",
    "\n",
    "# either t5-small,t5-base, t5-large, facebook/bart-base, or facebook/bart-large\n",
    "modelbase = 'facebook/bart-base'\n",
    "\n",
    "# we will use the above variables to set up the folder to save our model\n",
    "pre_trained_model_name = modelbase.split(\n",
    "    '/')[1] if 'bart' in modelbase else modelbase\n",
    "\n",
    "# where the trained model will be saved\n",
    "output_path = 'TrainModels/' + modeltype + '/'+pre_trained_model_name+'/'\n",
    "\n",
    "#tests = json.load(open(test_path,encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 453\n"
     ]
    }
   ],
   "source": [
    "rand_seed = 453\n",
    "seed_everything(rand_seed)\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "arguments = train_arguments = {'output_dir': output_path,\n",
    "                               'warmup_ratio': 0.2,\n",
    "                               #'disable_tqdm':False,\n",
    "                               'per_device_train_batch_size': 8,\n",
    "                               'num_train_epochs': 4,\n",
    "                               'lr_scheduler_type': 'cosine',\n",
    "                               'learning_rate': 5e-5,\n",
    "                               'evaluation_strategy': 'steps',\n",
    "                               'logging_steps': 500,\n",
    "                               \n",
    "                               'seed': rand_seed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NarratorUtils:\n",
    "    def __init__(self, modelbase, trained_model_path):\n",
    "        self.modelbase = modelbase\n",
    "        self.trained_model_path = trained_model_path\n",
    "        # Setting up tokenizer\n",
    "        self.tokenizer_ = setupTokenizer(modelbase)\n",
    "\n",
    "        self.local_dict = SimpleNamespace(modelbase=modelbase,\n",
    "                                          tokenizer_=self.tokenizer_)\n",
    "        self.setup_performed = False\n",
    "\n",
    "        print(' Dont forget to call initialise_Model() before running any inference')\n",
    "\n",
    "    def initialise_Model(self):\n",
    "        classification_explanator = get_basic_model(self.local_dict)()\n",
    "        # Set up the model along with the tokenizers and other important stuff required to run the generation\n",
    "        params_dict = json.load(open(self.trained_model_path+'/parameters.json'))\n",
    "        #state_dict = json.load(open(args.model_base_dir+'/parameters.json'))\n",
    "        best_check_point = params_dict['best_check_point']\n",
    "        best_check_point_model = best_check_point + '/pytorch_model.bin'\n",
    "\n",
    "        state_dict = torch.load(best_check_point_model)\n",
    "        classification_explanator.load_state_dict(state_dict)\n",
    "\n",
    "        classification_explanator.eval();\n",
    "        self.setup_performed = True\n",
    "\n",
    "        return classification_explanator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dont forget to call initialise_Model() before running any inference\n"
     ]
    }
   ],
   "source": [
    "\n",
    "narrator_utils = NarratorUtils(modelbase,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the model\n",
    "classification_explanator = narrator_utils.initialise_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import random\n",
    "# Example of input\n",
    "\n",
    "ml_task_name = 'Car Insurance Risk'\n",
    "\n",
    "prediction_probabilities = {'Low': 0.76, 'High': 0.24}\n",
    "# the features used to make the prediction\n",
    "feature_names = ['Height', 'Mar_status', 'cur_loc', 'nb_friends', 'last_trip']\n",
    "\n",
    "bcc = feature_names.copy()\n",
    "random.shuffle(bcc)\n",
    "\n",
    "# get the order and directions of influence from the explainable output from the XAI technique\n",
    "# the methods expects the keys ['explanation_order','positives','negatives','ignore']\n",
    "# 'positives' is the list of all the features with positive influence on the prediction decision and 'negatives' is the inverse.\n",
    "# 'ignore' is the list of features identified as having very limited contribution to the prediction decision\n",
    "\n",
    "attributions = {'explanation_order': ['Height', 'last_trip', 'cur_loc', 'nb_friends', 'Mar_status'],\n",
    "                'positives': ['Height', 'last_trip', 'Mar_status'],\n",
    "                'negatives': ['cur_loc', 'nb_friends'],\n",
    "                'ignore': []\n",
    "                \n",
    "                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to generate the texts via the iterative generation then we have to define the style \n",
    "# We want our output text to first table about the prediction output\n",
    "# step 1: talk about the feature order based on the attributions\n",
    "# step 2: talk about the features with positive contributions to the decision\n",
    "# step 3: ----- negative features\n",
    "# step 4: ------- features with limited influence\n",
    "# step 5: Make conclusion based on all the input information\n",
    "\n",
    "# this will instruct the narrator to follow our desired output style\n",
    "iterative_generation_steps = {'step 0': '',\n",
    "                    'step 1': attributions['explanation_order'],\n",
    "                    'step 2': attributions['positives'],\n",
    "                    'step 3': attributions['negatives'],\n",
    "                    'step 4': attributions['ignore'],\n",
    "                    'step 5': '-'\n",
    "                    }\n",
    "\n",
    "\n",
    "full_text_generation_steps = {'step 0':''}\n",
    "\n",
    "\n",
    "generation_instruction = iterative_generation_steps if iterative_gen else full_text_generation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Height', 'Mar_status', 'cur_loc', 'nb_friends', 'last_trip']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def inferenceIterativeGenerator(pack, \n",
    "                               pr_c=1,\n",
    "                               ignore=False,\n",
    "                               force_section=False,\n",
    "                               include_full_set=False,\n",
    "                               ):\n",
    "    pack = pack_x = copy.deepcopy(pack)\n",
    "    pack['narration'] = ' ,'.join([''.join(t) for t in  pack['steps']])\n",
    "    outputs = pack['steps']\n",
    "\n",
    "    max_init = 1\n",
    "    results = []\n",
    "\n",
    "    if len(outputs[max_init:]) > 0:\n",
    "        sofar = ' '.join(outputs[:max_init])+' [N1S]'+' '\n",
    "        pack_x = copy.deepcopy(pack)\n",
    "\n",
    "        pack_x['next_sequence'] = sofar\n",
    "        pack_x['prev_seq'] = '<prem>'\n",
    "        results.append(pack_x)\n",
    "    prev = copy.deepcopy(sofar)\n",
    "\n",
    "    for idx, sent in enumerate(outputs[max_init:-1]):\n",
    "        # print(idx+1)\n",
    "        lotto = [0, 1, 1, 0, 0, 1]\n",
    "        random.shuffle(lotto)\n",
    "        pack_x = copy.deepcopy(pack)\n",
    "\n",
    "        pack_x['next_sequence'] = sent+f' [N{idx+2}S]'\n",
    "        pack_x['prev_seq'] = sofar\n",
    "        results.append(pack_x)\n",
    "\n",
    "        sofar += sent+f' [N{idx+2}S]'+' '\n",
    "    pack_x = copy.deepcopy(pack)\n",
    "    pack_x['prev_seq'] = sofar+' [EON]'\n",
    "    pack_x['next_sequence'] = outputs[-1]+' [EON]'\n",
    "    results.append(pack_x)\n",
    "\n",
    "    if include_full_set:\n",
    "        pack_f = copy.deepcopy(pack)\n",
    "        pack_f['prev_seq'] = '<full_narration>' \n",
    "        pack_f['next_sequence'] = sofar+' [EON] '+ outputs[-1]+' [CON]' #outputs[-1]+' [EON]'\n",
    "        results.append(pack_f)\n",
    "    \n",
    "    results.append(sofar+' [EON] ')\n",
    "    return results\n",
    "\n",
    "def processFeatureAttributions(attributions, narration, force_consistency=True, nb_base=None):\n",
    "\n",
    "    #  nb_base: specifies the number of features to consider in the output text when full-text generation mode is used\n",
    "\n",
    "    feature_division = attributions\n",
    "    contradict, support, ignore = feature_division[\n",
    "        'negatives'], feature_division['positives'], feature_division['ignore']\n",
    "\n",
    "    output = {'features': [], 'order': [], 'direction': []}\n",
    "    nat = set([c.strip() for c in word_tokenize(narration)])\n",
    "\n",
    "    ordered_features = attributions['explanation_order']\n",
    "\n",
    "    for pos,feat in enumerate(ordered_features):\n",
    "        include = False\n",
    "\n",
    "        # check if the user specified if this feat should be in the output text\n",
    "        \n",
    "        if feat in nat and force_consistency:\n",
    "            include = True\n",
    "        elif feat not in nat and force_consistency:\n",
    "            include = False\n",
    "        elif not force_consistency:\n",
    "            include = True\n",
    "\n",
    "        if nb_base is not None and nb_features < nb_base:\n",
    "            include = True\n",
    "        if include:\n",
    "            output['features'].append(feat)\n",
    "            output['order'].append(pos)\n",
    "            if feat in contradict:\n",
    "                output['direction'].append(-1)\n",
    "            elif feat in support:\n",
    "                output['direction'].append(1)\n",
    "            else:\n",
    "                output['direction'].append(-2)\n",
    "        else:\n",
    "            pass\n",
    "    return output \n",
    "\n",
    "\n",
    "def  lineariseExplanation(data,\n",
    "    attributions, randomise_preds=False, force_consistency=True, shrink=None):\n",
    "    instance = data\n",
    "    narration = cleanNarrations(copy.deepcopy(instance['next_sequence']))\n",
    "    prev_seq = cleanNarrations(copy.deepcopy(instance['prev_seq']))\n",
    "    full_narra = cleanNarrations(copy.deepcopy(instance['narration']))\n",
    "\n",
    "    if len(narration) < 1:\n",
    "        force_consistency = False\n",
    "    feature_ranks =processFeatureAttributions(attributions, narration+prev_seq+full_narra, force_consistency=force_consistency)\n",
    "    feature_ranks2 = processFeatureAttributions(attributions, narration, force_consistency=force_consistency)\n",
    "    #processFeatureAttributions(attributions,nars[-1],force_consistency=iterative_gen)\n",
    "\n",
    "    feature_desc, [pf, nf, neu_f], directions = linearisedFeaturesAttributions(\n",
    "        feature_ranks, shrink=shrink)\n",
    "    feature_desc2, [pf2, nf2, neu_f2], directions2 = linearisedFeaturesAttributions(\n",
    "        feature_ranks2, shrink=shrink)\n",
    "\n",
    "    preamble, place_holderss = processPredictionProbabilities2(instance['prediction_confidence_level'],\n",
    "                                                               instance['predicted_class'], randomise=randomise_preds)\n",
    "\n",
    "    preamble = preamble+' <|section-sep|> '+feature_desc\n",
    "\n",
    "    class_labels = getClassLabels(7)\n",
    "    class_dict = {f'C{i+1}': c for i, c in enumerate(class_labels)}\n",
    "    class_dict['C1 or C2'] = '#CA or #CB'\n",
    "    class_dict['C2 or C1'] = '#CB or #CA'\n",
    "    class_dict.update({f'c{i+1}': c for i, c in enumerate(class_labels)})\n",
    "\n",
    "    class_dict = place_holderss\n",
    "\n",
    "    # [functools.reduce(lambda a, kv: a.replace(*kv), class_dict.items(),re.sub('\\s+', ' ', ss.strip().replace('\\n', ' '))) for ss in [preamble]][0]\n",
    "    extended1 = preamble\n",
    "    narr, prev = [functools.reduce(lambda a, kv: a.replace(*kv), class_dict.items(),\n",
    "                                   re.sub('\\s+', ' ', ss.strip().replace('\\n', ' '))) for ss in [narration, prev_seq]]\n",
    "\n",
    "    return {'rele_feat': [pf2, nf2, neu_f2], 'directions': directions, 'label_placeholders': place_holderss, 'preamble': extended1, 'narration': narr, 'prev_seq': prev,\n",
    "            'positives': pf, 'negatives': nf, 'neutral': neu_f, 'pred_label': class_dict[instance['predicted_class']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ['Height', 'last_trip', 'cur_loc', 'nb_friends', 'Mar_status'],\n",
       " ['Height', 'last_trip', 'Mar_status'],\n",
       " ['cur_loc', 'nb_friends'],\n",
       " [],\n",
       " '-']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(generation_instruction.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nars=inferenceIterativeGenerator({'steps':list([' '.join(t) for t in generation_instruction.values()]),\n",
    "'prediction_confidence_level': ', '.join([ f'{k}:{round(v*100,2)}% ' for k,v in prediction_probabilities.items()]),\n",
    "'predicted_class': \n",
    "\n",
    "},ignore= not iterative_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'steps': ['',\n",
       "  'Height last_trip cur_loc nb_friends Mar_status',\n",
       "  'Height last_trip Mar_status',\n",
       "  'cur_loc nb_friends',\n",
       "  '',\n",
       "  '-'],\n",
       " 'prediction_confidence_level': 'Low:76.0% , High:24.0% ',\n",
       " 'predicted_class': 'Low',\n",
       " 'narration': ',Height last_trip cur_loc nb_friends Mar_status,Height last_trip Mar_status,cur_loc nb_friends,,-',\n",
       " 'next_sequence': 'Height last_trip Mar_status [N3S]',\n",
       " 'prev_seq': ' [N1S] Height last_trip cur_loc nb_friends Mar_status [N2S] '}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nars[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfa = processFeatureAttributions(attributions,nars[-1],force_consistency=iterative_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': ['Height', 'last_trip', 'cur_loc', 'nb_friends', 'Mar_status'],\n",
       " 'order': [0, 1, 2, 3, 4],\n",
       " 'direction': [1, 1, -1, -1, 1]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplanationRecord():\n",
    "    def __init__(self,ml_task_name,feature_names,prediction_probabilities, attributions,iterative_mode=False) -> None:\n",
    "        self.input_record = SimpleNamespace()\n",
    "        classes = list(prediction_probabilities.keys())\n",
    "        self.input_record.classes = classes\n",
    "        self.input_record.feature_names= feature_names\n",
    "        self.input_record.attributions = attributions\n",
    "        self.input_record.ml_task_name = ml_task_name\n",
    "        self.input_record.prediction_as_string =  ' , '.join([ f'{k}:{round(v*100,2)}% ' for k,v in prediction_probabilities.items()])\n",
    "\n",
    "        predicted_label = max(prediction_probabilities, key=prediction_probabilities.get)\n",
    "        self.input_record.predicted_class = predicted_label\n",
    "        print(f\"The ML model predicted the label : {predicted_label}\")\n",
    "\n",
    "        self.iterative_mode = iterative_mode\n",
    "\n",
    "\n",
    "    def setup_generation_steps(self,generation_steps):\n",
    "        if not self.iterative_mode:\n",
    "            print(' The texts will be generated in the default full-text mode')\n",
    "        self.input_record.generation_steps_as_string = list([' '.join(t) for t in generation_steps.values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ML model predicted the label : Low\n"
     ]
    }
   ],
   "source": [
    "exp_record = ExplanationRecord(ml_task_name,feature_names,prediction_probabilities, attributions,iterative_mode=True)\n",
    "exp_record.setup_generation_steps(generation_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.NarrationDatautils_new import cleanNarrations\n",
    "\n",
    "\n",
    "cleanNarrations = lambda x: x\n",
    "nars=inferenceIterativeGenerator({'steps':exp_record.input_record.generation_steps_as_string,\n",
    "'prediction_confidence_level': exp_record.input_record.prediction_as_string,\n",
    "'predicted_class': exp_record.input_record.predicted_class\n",
    "\n",
    "},ignore= not iterative_gen)\n",
    "\n",
    "pfa = processFeatureAttributions(exp_record.input_record.attributions,nars[-1],force_consistency=iterative_gen)\n",
    "\n",
    "instance = nars[3]\n",
    "narration = cleanNarrations(copy.deepcopy(instance['next_sequence']))\n",
    "prev_seq = cleanNarrations(copy.deepcopy(instance['prev_seq']))\n",
    "full_narra = cleanNarrations(copy.deepcopy(instance['narration']))\n",
    "feature_ranks =processFeatureAttributions(exp_record.input_record.attributions, narration+prev_seq+full_narra, force_consistency=True)\n",
    "feature_ranks2 = processFeatureAttributions(exp_record.input_record.attributions, narration, force_consistency=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'steps': ['',\n",
       "  'Height last_trip cur_loc nb_friends Mar_status',\n",
       "  'Height last_trip Mar_status',\n",
       "  'cur_loc nb_friends',\n",
       "  '',\n",
       "  '-'],\n",
       " 'prediction_confidence_level': 'Low:76.0% , High:24.0% ',\n",
       " 'predicted_class': 'Low',\n",
       " 'narration': ',Height last_trip cur_loc nb_friends Mar_status,Height last_trip Mar_status,cur_loc nb_friends,,-',\n",
       " 'next_sequence': 'cur_loc nb_friends [N4S]',\n",
       " 'prev_seq': ' [N1S] Height last_trip cur_loc nb_friends Mar_status [N2S] Height last_trip Mar_status [N3S] '}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': ['Height', 'last_trip', 'cur_loc', 'nb_friends', 'Mar_status'],\n",
       " 'order': [0, 1, 2, 3, 4],\n",
       " 'direction': [1, 1, -1, -1, 1]}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': ['cur_loc', 'nb_friends'], 'order': [2, 3], 'direction': [-1, -1]}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_ranks2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('annotation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4aea246828f75a58a93204fce55d322b87a38415c2742fb8a88040418150f4d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
